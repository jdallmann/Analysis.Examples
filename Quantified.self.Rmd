---
title: "The Quantified Self:  \nProper Lifting"
author: "Justin Dallmann"
date: "8/18/2017"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(include = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

```{r load, include=FALSE, echo=FALSE}
library(caret)
library(dplyr)
library(ggplot2)
```


# Executive summary
Measurements from belt, arm, dumbbell, and forearm monitors are used to predict whether dumbbell bicep curls have been performed correctly. In what follows, I develop a prediction model for whether or a curl has been performed correctly using random forests with 10 fold cross-validation (via the `caret` package). The training set accuracy of the final model is 99.3%, while the out of sample error estimated on a large hold out set is 99.6%.

The data used to build the prediction model comes from [Groupware@LES](http://groupware.les.inf.puc-rio.br/har) research group. The data set contains 19,622 observations of 159 variables in which 6 participants from 20 to 28 years of age were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five ways: (i) exactly according to the specification, (ii) throwing the elbows to the front, (iii) lifting the dumbbell only halfway, (iv) lowering the dumbbell only halfway, and (v) throwing the hips to the front (Velloso et. al. 2013, p. 3). As they note, though much work has been done in the domain of automated activity recognition, automated *quality* of activity recognition has received considerably less attention. For further details, see:

- Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. 'Qualitative Activity Recognition of Weight Lifting Exercises'. *Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13)* . Stuttgart, Germany: ACM SIGCHI, 2013.



# Loading the data
```{r loadData}
# Load data
data <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
                 na.strings=c("NA","","#DIV/0!"))
```



# Cleaning the data
In order to build the model I remove remove time and date information, and summary statistic variables for each measurement window (to ensure that the window identifier information is not used in prediction). The data is then randomly separated into a training set (80% of data) upon which to perform 10-fold cross-validation model selection and a hold out set (20% of data) to estimate out of sample error.

```{r cleanData}
# Get rid of date-time info, window information, and subject identifiers
data <- data[,8:160]

# Get rid of NA value columns
data <- data[ , colSums(is.na(data)) == 0]


# Set training set and test set
set.seed(1985)
inTrain <- createDataPartition(y=data$classe, p=0.80, list = FALSE) 
train <- data[inTrain,]
test <- data[-inTrain,]
```


# Modeling
To create the final prediction model, I used a random forest method with 10-fold cross-validation on all remaining variables.
```{r firstPass}
# Train the model
# [NB: the CARET package already takes care of 
# n-fold cross-validation for random forests.]
forMod <- train(classe~., data=train, method="rf", 
            trControl=trainControl(method="cv",number=10),
            prox=TRUE, allowParallel=TRUE)
```





## Summary of results
Of the models explored, the best model had an in training set 10-fold cross-validation error rate of 0.7% (accuracy of 99.3%) using random samples of 2 variables (`mtry=2`). The error in the hold-out set is (surprisingly) higher still at 99.6% and offers a good prediction of out of sample accuracy.

```{r testPred}
# Print details from the model training
print(forMod)

# Print training set accuracy
print(forMod$finalModel)

# Print test set accuracy
testPreds <- predict(forMod, test[,-53])
confusionMatrix(testPreds, test$classe)
```

The most important variables (by Gini importance) were `roll_belt`, `pitch_forearm`, `yaw_belt`, `magnet_dumbell_z`, `magnet_dumbell_y`, `pitch_belt`, and `roll_forearm`, with importance trailing off substantially afterwards for the others.

```{r predVisualization, echo=FALSE}
varImpPlot(forMod$finalModel, n.var=10)
```

# Suggestions for further exploration
If greater accuracy were required, there are a couple of further approaches that might be worth trying. For example:

1. Tune the random forest model more systematically for other numbers of variables sampled  (other values of `mtry`).
2. Generating further features, including the summary statistics that were originally removed from the data set for the analysis. Costs of this approach include increasing computational time, increasing the chances of over-fitting.
3. Including time-series information. Costs of this approach include increasing computational time, and potentially adding spurious correlations across  measurements.
4. Adding within-subject estimation of quality of action. Costs of this approach include increasing computational time and reducing availability (since measurements for each new individual would be needed to calibrate the model).
5. Trying different methods like logistic regression, neural network predictors, or some predictor that aggregates the above approaches. This option would most increase analysis time and computational time, though would probably secure the greatest gains especially if combined with the above suggestions.