---
title: "The Quantified Self:  \nProper Lifting"
author: "Justin Dallmann"
date: "8/18/2017"
output: 
    pdf_document:
        toc: TRUE
        number_sections: TRUE
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(include = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(comment = "|")
```

```{r load, include=FALSE, echo=FALSE}
library(dplyr)
library(caret)
library(randomForest)
```


# Executive summary
Measurements from belt, arm, dumbbell, and forearm monitors are used to predict whether dumbbell bicep curls have been performed correctly. In what follows, I develop a prediction model for whether or a curl has been performed correctly using random forests with 10 fold cross-validation (via the `caret` package). The training set accuracy of the final model is 99.3%, while the out of sample error estimated on a large hold out set is 99.6%. (Other prediction models explored, and their accuracy estimates, can be seen in the last section.)

The data used to build the prediction model comes from [Groupware@LES](http://groupware.les.inf.puc-rio.br/har) research group. The data set contains 19,622 observations of 159 variables in which 6 participants from 20 to 28 years of age were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five ways: (i) exactly according to the specification, (ii) throwing the elbows to the front, (iii) lifting the dumbbell only halfway, (iv) lowering the dumbbell only halfway, and (v) throwing the hips to the front (Velloso et. al. 2013, p. 3). As they note, though much work has been done in the domain of automated activity recognition, automated *quality* of activity recognition has received considerably less attention. For further details, see:

- Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. 'Qualitative Activity Recognition of Weight Lifting Exercises'. *Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13)* . Stuttgart, Germany: ACM SIGCHI, 2013.



# Loading the data
```{r loadData}
# Load data
data <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
                 na.strings=c("NA","","#DIV/0!"))
```



# Cleaning the data
In building the final model I remove remove time and date information, and summary statistic variables for each measurement window (to ensure that the window identifier information is not used in prediction). The data is then randomly separated into a training set (80% of data) upon which to perform 10-fold cross-validation model selection and a hold out set (20% of data) to estimate out of sample error.

```{r cleanData}
# Get rid of date-time info, window information, and subject identifiers
data <- data[,8:160]

# Get rid of NA value columns
data <- data[ , colSums(is.na(data)) == 0]

# Set training set and test set
set.seed(1980)
inTrain <- createDataPartition(y=data$classe, p=0.80, list = FALSE) 
train <- data[inTrain,]
test <- data[-inTrain,]
```


# Modeling
To create the final prediction model, I used a random forest method with 10-fold cross-validation on all remaining variables.
```{r firstPass}
## train random forest model
## [NB: the CARET package already takes care of 
## n-fold cross-validation for random forests.]
# set.seed(1981)
# forMod <- train(classe~., data=train, method="rf", 
#             trControl=trainControl(method="cv",number=10),
#             prox=TRUE, allowParallel=TRUE)
#
# save(forMod,file="forMod.RData")

load("forMod.RData")
```





## Summary of results
Of the models explored, the best model had an in training set 10-fold cross-validation error rate of 0.7% (accuracy of 99.3%) using random samples of 27 variables (`mtry=27`). The accuracy in the hold-out set is (surprisingly) higher still at 99.6% and offers a good prediction of out of sample accuracy.

```{r testPred1}
# Print details from the model training
print(forMod)
```

\vspace{.5cm}

```{r testPred2}
# Print training set accuracy
print(forMod$finalModel)
```

\vspace{.5cm}

```{r testPred3}
# Print test set accuracy
testPreds <- predict(forMod, test[,-53])
confusionMatrix(testPreds, test$classe)
```

The most important variables (by Gini importance) were `roll_belt`, `pitch_forearm`, `yaw_belt`, `magnet_dumbell_z`, `magnet_dumbell_y`, `pitch_belt`, and `roll_forearm`, with importance trailing off substantially afterwards for the others.

```{r predVisualization, echo=FALSE}
varImpPlot(forMod$finalModel, n.var=10, main="Importance of variables")
```

# Other models explored and suggestions for further tuning
## Recusive partitioning/CART
In exploration, I also fit a recursive partitioning CART model (Breiman et. at., 1984). With a best model out of sample accuracy rate ~88% (tuned for complexity values of cp in [.0005, .05] and default 10 cross-validations).

\pagebreak
```{r recPartModel}
## train CART model
#
# set.seed(1982)
# treeMod <- train(classe ~ ., data=train, 
#               method="rpart",
#               control = rpart.control(minsplit = 100),
#               tuneGrid = data.frame(cp = 0.001)) 
#
# save(treeMod,file="treeMod.RData")

load("treeMod.RData")
```

\vspace{.5cm}

```{r printRecPartModel}
# print test set accuracy
treeModPreds <- predict(treeMod$finalModel, 
                        test[,-53], type="class")
confusionMatrix(treeModPreds, test$classe)
```

## Gradient boosting
Boosting with trees (gradient boosting machine) has a best model out of sample accuracy rate ~96%. The final values used for the model were n.trees = 150, interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10. No further tuning of shrinkage for the model was performed.
```{r boostModel}
## train gbm
# 
# set.seed(1983)
# gbmMod <- train(classe ~ ., data=train, 
#               method="gbm", verbose = FALSE,
#               trControl=
#                   trainControl(method="cv",number=10))
# gbmMod
# save(gbmMod,file="gbmMod.RData")

load("gbmMod.RData")
```

```{r gbmFig1}
# print test set accuracy
gbmModPreds <- predict(gbmMod, test[,-53])
confusionMatrix(gbmModPreds, test$classe)
```

```{r gbmFig2, echo = FALSE}
plot(gbmMod)
```

## Further tuning
If greater accuracy were required, there are a couple of further approaches that might be worth trying. For example:

1. Tune the random forest model more systematically for other numbers of variables sampled  (other values of `mtry`).
2. Generating further features, including the summary statistics that were originally removed from the data set for the analysis. Costs of this approach include increasing computational time, increasing the chances of over-fitting.
3. Including time-length of activity information. Costs of this approach include increasing computational time, and potentially adding spurious correlations across  measurements. Following this approach it would also be important to measure time **from the begining of each exercise window**---since if the full timestamp were used, knowledge of which time slice the action is being performed in would provide illicit near perfect information regarding the class of exercise being undertaken. 
4. Adding within-subject estimation of quality of action. Costs of this approach include increasing computational time and reducing availability (since measurements for each new individual would be needed to calibrate the model).
5. Trying different methods like logistic regression, neural network predictors, or some predictor that aggregates the above approaches. This option would most increase analysis time and computational time, though would probably secure the greatest gains especially if combined with the above suggestions.